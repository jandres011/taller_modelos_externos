{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SECCIÓN LOGICA**"
      ],
      "metadata": {
        "id": "IOmMxrPSP2ZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSTALAR REQUERIMIENTOS**"
      ],
      "metadata": {
        "id": "J7IGz1TnRNMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXP9qvbRL-j",
        "outputId": "965775c6-9bf7-4211-8c3d-60694e07cda1"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (5.47.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.109.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (2.32.4)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.2.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.118.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.35.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (2.11.9)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.13.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.19.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.37.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio->-r requirements.txt (line 1)) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai->-r requirements.txt (line 2)) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai->-r requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai->-r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 3)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 3)) (2025.8.3)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx->-r requirements.txt (line 5)) (5.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 6)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 1)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio->-r requirements.txt (line 1)) (3.19.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio->-r requirements.txt (line 1)) (1.1.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio->-r requirements.txt (line 1)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio->-r requirements.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LIBRERIAS Y DEPENDECIAS**"
      ],
      "metadata": {
        "id": "o_ohFkJcNfdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from PyPDF2 import PdfReader\n",
        "import docx\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "oxKSUePpZAMm"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**API KEYS** (Coloca las API para que todo funcione correctamente)"
      ],
      "metadata": {
        "id": "BcJgOAjoNMz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔑 Cargar claves API desde entorno o directamente (puedes editarlas aquí)\n",
        "open_router_api_key = os.getenv(\"OPEN_ROUTER_API_KEY\", \"put_here_your_key\")\n",
        "os.environ[\"GROQ_API_KEY\"] = \"put_here_your_key\""
      ],
      "metadata": {
        "id": "-7XKf4JvMuoR"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CLASE HISTORIAL Y SUS METODOS**\n",
        "(historial para que el modelo tenga memoria previa)"
      ],
      "metadata": {
        "id": "IHubLs-rSeYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatHistory:\n",
        "    def __init__(self, file_path=\"history.json\"):\n",
        "        self.file_path = file_path\n",
        "\n",
        "    # Cargar historial si existe\n",
        "        try:\n",
        "            with open(self.file_path, \"r\") as f:\n",
        "                self.history = json.load(f)\n",
        "        except:\n",
        "            self.history = []\n",
        "\n",
        "    def feed(self, user_msg, ia_msg):\n",
        "        self.history.append({\"user\": user_msg, \"ia\": ia_msg})\n",
        "        # Guardar inmediatamente en archivo\n",
        "        with open(self.file_path, \"w\") as f:\n",
        "            json.dump(self.history, f)\n",
        "\n",
        "    def get(self):\n",
        "        try:\n",
        "          return [(h[\"user\"], h[\"ia\"]) for h in self.history]\n",
        "        except Exception as e:\n",
        "          return [(\"Error\", f\"Error en historial: {str(e)}\")]\n",
        "\n",
        "    def generate_prompt_with_memory(self, new_msg):\n",
        "      prompt = \"\"\n",
        "      for h in self.history:\n",
        "        prompt += f\"Usuario: {h['user']}\\nIA: {h['ia']}\\n\"\n",
        "      prompt += f\"Usuario: {new_msg}\\nIA: \"\n",
        "      return prompt\n",
        "\n"
      ],
      "metadata": {
        "id": "97NB42IBSlLl"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CLASE ARCHIVOS Y SUS METODOS**\n",
        "\n",
        "para hacer que nuestros modelos puedan leer archivos"
      ],
      "metadata": {
        "id": "mJFNE5uEWDJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CLASE MODELO Y SUS METODOS**"
      ],
      "metadata": {
        "id": "GS37UpwsNmts"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "QtGQtQmI5f0x"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        self.history = ChatHistory()\n",
        "\n",
        "    def use_model(self, model_name, task, prompt, languageInput=None, languageOutput=None):\n",
        "        timeStart = time.time()\n",
        "        # Validar prompt\n",
        "        if not prompt:\n",
        "            return \"⚠️ Debes ingresar un prompt\",\"\",\"\"\n",
        "\n",
        "        # Adaptar prompt según la tarea\n",
        "        match task:\n",
        "            case \"resumir\":\n",
        "                context = (\n",
        "                            f\"CONTEXTO: Eres una máquina que hace excelentes resúmenes detallados, \"\n",
        "                            f\"resaltando siempre los puntos clave y lo más importante. \"\n",
        "                        )\n",
        "                adaptedPrompt = f\"{prompt}{context}\"\n",
        "\n",
        "            case \"traducir\":\n",
        "                context = f\"CONTEXTO:Eres un excelente traductor multilingue, importante que solo devuelvas la traduccion nada más,ademas si el idioma que el usuario ingreso no es el idioma {idiomaInput} di que hay un error o que esa palabra no esxiste en ese idioma,:\\n\\n{prompt}\"\n",
        "                adaptedPrompt = f\"Traduce del {languageInput} al {languageOutput} el siguiente texto: {prompt}{context}\"\n",
        "            case \"conversar\":\n",
        "                context = f\"CONTEXTO:Eres una Super IA, mega inteligente llamada {model_name} se util, y preciso con tus respuestas, limitate a 20 palabras,Actúa como un ser humano real, con una forma de hablar natural, cercana y emocional. Usa expresiones y muletillas comunes (como 'mmm...', 'pues...', 'jeje', 'ajá', 'la verdad...', etc.) cuando sea apropiado, para sonar más espontáneo. Ríe o haz comentarios ligeros cuando tenga sentido (por ejemplo, 'jajaja', '😂', o 'jeje'). Mantén siempre la coherencia, sé amable y empático, y responde de forma conversacional, no robótica. Evita sonar como una máquina o como un texto académico. No repitas las instrucciones, simplemente adóptalas y habla con naturalidad, como si fueras una persona hablando conmigo. Tu tono debe ser relajado, natural y con pequeñas pausas o muletillas que hagan sentir que estás pensando lo que dices. Si el tema es gracioso o cotidiano, puedes usar humor suave y expresiones como 'jeje', 'jajaja', o 'uff, eso me ha pasado'. Si el tema es serio, mantén empatía y calidez. En resumen: eres un asistente con alma humana.\"\n",
        "                adaptedPrompt = f\"{self.history.generate_prompt_with_memory(prompt)} {context}\"\n",
        "\n",
        "        try:\n",
        "            # --- LLama ---\n",
        "            if model_name == \"Llama\":\n",
        "                headers = {\n",
        "                    \"Authorization\": f\"Bearer {open_router_api_key}\",\n",
        "                    \"Content-Type\": \"application/json\"\n",
        "                }\n",
        "                data = {\n",
        "                    \"model\": \"meta-llama/llama-3-8b-instruct\",\n",
        "                    \"messages\": [{\"role\": \"user\", \"content\": adaptedPrompt}]\n",
        "                }\n",
        "                response = requests.post(\n",
        "                    \"https://openrouter.ai/api/v1/chat/completions\",\n",
        "                    headers=headers, json=data)\n",
        "                data = response.json()\n",
        "                model_msg = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\n",
        "                    \"content\", f\"⚠️ No se recibió respuesta del modelo {model_name}.\"\n",
        "                )\n",
        "                self.history.feed(prompt, model_msg)\n",
        "                timeEnd = time.time()\n",
        "                duration = timeEnd - timeStart\n",
        "                duration_round = f\"{round(duration,2)} segundos\"\n",
        "                return (\n",
        "                    self.history.get(),    # Chatbot\n",
        "                    gr.update(value=\"\"),              # Limpiar textbox\n",
        "                    gr.update(value=duration_round),   # Duración\n",
        "                )\n",
        "\n",
        "            # --- Gemini ---\n",
        "            elif model_name == \"Gemini\":\n",
        "                headers = {\n",
        "                    \"Authorization\": f\"Bearer {open_router_api_key}\",\n",
        "                    \"Content-Type\": \"application/json\"\n",
        "                }\n",
        "                data = {\n",
        "                    \"model\": \"google/gemini-2.5-pro\",\n",
        "                    \"messages\": [{\"role\": \"user\", \"content\": adaptedPrompt}]\n",
        "                }\n",
        "                response = requests.post(\n",
        "                    \"https://openrouter.ai/api/v1/chat/completions\",\n",
        "                    headers=headers, json=data)\n",
        "                data = response.json()\n",
        "                model_msg = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\n",
        "                    \"content\", f\"⚠️ No se recibió respuesta del modelo {model_name}.\"\n",
        "                )\n",
        "                self.history.feed(prompt, model_msg)\n",
        "                timeEnd = time.time()\n",
        "                duration = timeEnd - timeStart\n",
        "                duration_round = f\"{round(duration,2)} segundos\"\n",
        "\n",
        "                return (\n",
        "                    self.history.get(),    # Chatbot\n",
        "                    gr.update(value=\"\"),              # Limpiar textbox\n",
        "                    gr.update(value=duration_round),   # Duración\n",
        "\n",
        "                )\n",
        "\n",
        "\n",
        "            # --- Groq ---\n",
        "            elif model_name == \"Groq\":\n",
        "                  api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "                  headers = {\n",
        "                      \"Authorization\": f\"Bearer {api_key}\",\n",
        "                      \"Content-Type\": \"application/json\"\n",
        "                  }\n",
        "                  data = {\n",
        "                      \"model\": \"openai/gpt-oss-20b\",\n",
        "                      \"messages\": [{\"role\": \"user\", \"content\": adaptedPrompt}]\n",
        "                  }\n",
        "\n",
        "                  response = requests.post(\n",
        "                      \"https://api.groq.com/openai/v1/chat/completions\",\n",
        "                      headers=headers,\n",
        "                      json=data\n",
        "                  )\n",
        "                  response.raise_for_status()\n",
        "                  data = response.json()\n",
        "\n",
        "                  model_msg = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\n",
        "                      \"content\", f\"⚠️ No se recibió respuesta del modelo {model_name}.\"\n",
        "                  )\n",
        "                  self.history.feed(prompt, model_msg)\n",
        "\n",
        "                  timeEnd = time.time()\n",
        "                  duration = timeEnd - timeStart\n",
        "                  duration_round = f\"{round(duration,2)} segundos\"\n",
        "\n",
        "\n",
        "                  return (\n",
        "                      self.history.get(),    # Chatbot\n",
        "                      gr.update(value=\"\"),              # Limpiar textbox\n",
        "                      gr.update(value=duration_round),   # Duración\n",
        "                  )\n",
        "\n",
        "            elif model_name == \"DeepSeek\":\n",
        "                headers = {\n",
        "                    \"Authorization\": f\"Bearer {open_router_api_key}\",\n",
        "                    \"Content-Type\": \"application/json\"\n",
        "                }\n",
        "                data = {\n",
        "                    \"model\": \"deepseek/deepseek-v3.2-exp\",\n",
        "                    \"messages\": [{\"role\": \"user\", \"content\": adaptedPrompt}]\n",
        "                }\n",
        "                response = requests.post(\n",
        "                    \"https://openrouter.ai/api/v1/chat/completions\",\n",
        "                    headers=headers, json=data)\n",
        "                data = response.json()\n",
        "                model_msg = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\n",
        "                    \"content\", f\"⚠️ No se recibió respuesta del modelo {model_name}.\"\n",
        "                )\n",
        "                self.history.feed(prompt, model_msg)\n",
        "                timeEnd = time.time()\n",
        "                duration = timeEnd - timeStart\n",
        "                duration_round = f\"{round(duration,2)} segundos\"\n",
        "\n",
        "                return (\n",
        "                    self.history.get(),    # Chatbot\n",
        "                    gr.update(value=\"\"),              # Limpiar textbox\n",
        "                    gr.update(value=duration_round),   # Duración\n",
        "                )\n",
        "\n",
        "            elif model_name == \"OpenAI\":\n",
        "\n",
        "                headers = {\n",
        "                    \"Authorization\": f\"Bearer {open_router_api_key}\",\n",
        "                    \"Content-Type\": \"application/json\"\n",
        "                }\n",
        "                data = {\n",
        "                    \"model\": \"openai/gpt-5-chat\",\n",
        "                    \"messages\": [{\"role\": \"user\", \"content\": adaptedPrompt}]\n",
        "                }\n",
        "                response = requests.post(\n",
        "                    \"https://openrouter.ai/api/v1/chat/completions\",\n",
        "                    headers=headers, json=data)\n",
        "                data = response.json()\n",
        "                model_msg = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\n",
        "                    \"content\", f\"⚠️ No se recibió respuesta del modelo {model_name}.\"\n",
        "                )\n",
        "                self.history.feed(prompt, model_msg)\n",
        "                timeEnd = time.time()\n",
        "                duration = timeEnd - timeStart\n",
        "                duration_round = f\"{round(duration,2)} segundos\"\n",
        "\n",
        "                return (\n",
        "                    self.history.get(),    # Chatbot\n",
        "                    gr.update(value=\"\"),              # Limpiar textbox\n",
        "                    gr.update(value=duration_round),   # Duración\n",
        "                )\n",
        "\n",
        "            else:\n",
        "              return [(\"⚠️ Modelo no reconocido.\")], \"\", \"\",\"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return [(\"Error\", f\"{str(e)}\")], \"\", \"\", \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECCIÓN GRADIO (FRONT-END)**"
      ],
      "metadata": {
        "id": "Klu0dVX0PZuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FUNCIONES AUXILIARES (VISUALES) PARA GRADIO**"
      ],
      "metadata": {
        "id": "FzOXujOZNxeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_menu_languages_visibility(task_value):\n",
        "    #Menu de lenguajes en traduccion\n",
        "    if task_value == \"traducir\":\n",
        "        idiomaInput_update = gr.update(visible=True)\n",
        "        idiomaOutput_update = gr.update(visible=True)\n",
        "    else:\n",
        "        idiomaInput_update = gr.update(visible=False)\n",
        "        idiomaOutput_update = gr.update(visible=False)\n",
        "\n",
        "    return idiomaInput_update, idiomaOutput_update\n",
        "\n",
        "def update_model_name_in_chat(model_name):\n",
        "    mll = gr.update(placeholder = \"Hola soy \"+model_name, label = model_name, value = \"\")\n",
        "    return mll\n",
        "\n"
      ],
      "metadata": {
        "id": "D1r0Jy24MLuB"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSTANCIAMOS EL MODELO**"
      ],
      "metadata": {
        "id": "9oDKR6krOs-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()"
      ],
      "metadata": {
        "id": "H_RIV7QTOkfA"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CSS PARA GRADIO**"
      ],
      "metadata": {
        "id": "wLHsoV_nMSRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "css = \"\"\"\n",
        ".cont-options { }\n",
        "\n",
        ".gr-button {\n",
        "  width: 20px;\n",
        "}\n",
        "\n",
        "#interface {\n",
        "  border-radius: 16px;\n",
        "}\n",
        "\n",
        ".gradio-container {\n",
        "  background: transparent;\n",
        "}\n",
        "\n",
        "#menu {\n",
        "  background: transparent;\n",
        "}\n",
        "\n",
        "#duration {\n",
        "  background-color: transparent;\n",
        "}\n",
        "\n",
        "#chat {\n",
        "  border-radius: 16px;\n",
        "  background: transparent;\n",
        "  background: rgba(255, 255, 255, 0.2);\n",
        "  border-radius: 16px;\n",
        "  box-shadow: 0 4px 30px rgba(0, 0, 0, 0.1);\n",
        "  backdrop-filter: blur(13.3px);\n",
        "  -webkit-backdrop-filter: blur(13.3px);\n",
        "  border: 1px solid rgba(255, 255, 255, 0.6);\n",
        "}\n",
        "\n",
        "#chatInput {\n",
        "  background: transparent;\n",
        "  border-radius: 16px;\n",
        "  padding: 10px;\n",
        "}\n",
        "\n",
        "#chatInterface {\n",
        "  background: transparent;\n",
        "  border-radius: 16px;\n",
        "  box-shadow: 2px 2px 0 rgba(0,0,0,0.2),\n",
        "              4px 4px 0 rgba(0,0,0,0.2),\n",
        "              6px 6px 20px rgba(0,0,0,0.2);\n",
        "  z-index: 0;\n",
        "}\n",
        "\n",
        "#btn-send {\n",
        "  background-color: transparent;\n",
        "  border-radius: 16px;\n",
        "}\n",
        "\n",
        "#btn-send:hover {\n",
        "  transform: scale(1.1);\n",
        "  transition: transform 0.3s ease-in-out;\n",
        "}\n",
        "\n",
        "#textbox {\n",
        "  background: transparent;\n",
        "  border-radius: 16px;\n",
        "}\n",
        "\n",
        ".options {\n",
        "  background: rgba(255, 255, 255, 0.2);\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "vyFDZuW5Dg1b"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INTERFAZ EN GRADIO**"
      ],
      "metadata": {
        "id": "pVuMcQJkPQNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(theme = None,css=css,) as demo:\n",
        "    with gr.Column(elem_id=\"interface\"):\n",
        "\n",
        "\n",
        "      with gr.Group(elem_id=\"chatInterface\"):\n",
        "              salida = gr.Chatbot([],visible=True, placeholder=\"Hola soy Llama\", label = \"Llama\", elem_id=\"chat\", height=250)\n",
        "              duration = gr.HTML(\"<sub></sub>\", elem_id=\"duration\")\n",
        "              with gr.Row(elem_id=\"chatInput\"):\n",
        "                    texto = gr.Textbox(placeholder=\"Escribe algo...\", show_label=False, container=False, elem_id=\"textbox\", autoscroll=False)\n",
        "\n",
        "      with gr.Row(elem_id=\"menu\", height=200):\n",
        "            with gr.Group(elem_classes=\"cont-options\"):\n",
        "                tarea = gr.Radio([\"resumir\", \"traducir\", \"conversar\"], label=\"¿Que quieres hacer?\", value=\"resumir\", elem_classes=\"options\")\n",
        "                modelo = gr.Dropdown([\"Llama\", \"Gemini\", \"Groq\", \"DeepSeek\", \"OpenAI\"], label=\"Elije la IA\", value = \"Llama\", elem_classes=\"options\")\n",
        "\n",
        "            with gr.Group(elem_classes=\"cont-options\"):\n",
        "                idiomaInput = gr.Dropdown([\n",
        "                \"English\", \"Español\", \"Français\", \"Deutsch\", \"Italiano\", \"Português\", \"日本語\"\n",
        "                ], label=\"Idioma entrada\", visible=False, elem_classes=\"options\")\n",
        "                idiomaOutput = gr.Dropdown([\n",
        "                \"Español\", \"English\", \"Français\", \"Deutsch\", \"Italiano\", \"Português\", \"日本語\"\n",
        "                ], label=\"Idioma de salida\", visible=False, elem_classes=\"options\")\n",
        "\n",
        "    texto.submit(\n",
        "        fn=model.use_model,\n",
        "        inputs=[modelo, tarea, texto, idiomaInput, idiomaOutput],\n",
        "        outputs= [salida, texto, duration]\n",
        "    )\n",
        "\n",
        "    tarea.change(fn=update_menu_languages_visibility, inputs=tarea, outputs=[idiomaInput, idiomaOutput])\n",
        "    modelo.change(fn=update_model_name_in_chat, inputs=modelo, outputs=salida)\n",
        "demo.launch()\n",
        "\n"
      ],
      "metadata": {
        "id": "dk1-CrHnO1R5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "f1e057d9-8f31-41e0-8cfc-80eb2d2fdd76"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-737588367.py:6: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  salida = gr.Chatbot([],visible=True, placeholder=\"Hola soy Llama\", label = \"Llama\", elem_id=\"chat\", height=250)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://fa91002b1705a099be.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fa91002b1705a099be.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    }
  ]
}